2024-06-20 00:34:29,651 - root - INFO - driver - main - creating spark session
2024-06-20 00:34:29,651 - Create_spark - INFO - create_spark - get_spark - started creating spark object
2024-06-20 00:34:38,350 - Create_spark - INFO - create_spark - get_spark - created spark session
2024-06-20 00:34:38,350 - root - INFO - driver - main - spark session has been created successfully
2024-06-20 00:34:38,350 - root - INFO - driver - main - trying to read source dataframe for cutomers
2024-06-20 00:34:38,350 - root - INFO - driver - main - trying to get schema for customer
2024-06-20 00:34:38,350 - Utils - INFO - utils - get_schema_from_json_file - trying to get schema for C:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\customers_schema\schema_customers.json
2024-06-20 00:34:41,484 - Utils - INFO - utils - get_schema_from_json_file - schema has beeen created forC:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\customers_schema\schema_customers.json
2024-06-20 00:34:41,485 - Utils - INFO - utils - get_source_dataframe - trying to create dataframe forC:\Users\saura\Desktop\pyspark_check\landing_zone\customers\customers.json
2024-06-20 00:34:41,524 - Utils - INFO - utils - get_source_dataframe - dataframe has been created for C:\Users\saura\Desktop\pyspark_check\landing_zone\customers\customers.json
2024-06-20 00:34:41,524 - Utils - INFO - utils - get_source_dataframe - dropping null records if any for C:\Users\saura\Desktop\pyspark_check\landing_zone\customers\customers.json
2024-06-20 00:34:41,542 - Utils - INFO - utils - get_source_dataframe - dropping null records if any has been completed for C:\Users\saura\Desktop\pyspark_check\landing_zone\customers\customers.json
2024-06-20 00:34:41,542 - Utils - INFO - utils - get_schema_from_json_file - trying to get schema for C:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\promotions_schema\schema_promotions.json
2024-06-20 00:34:41,621 - Utils - INFO - utils - get_schema_from_json_file - schema has beeen created forC:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\promotions_schema\schema_promotions.json
2024-06-20 00:34:41,621 - Utils - INFO - utils - get_source_dataframe - trying to create dataframe forC:\Users\saura\Desktop\pyspark_check\landing_zone\promotions\
2024-06-20 00:34:41,650 - Utils - INFO - utils - get_source_dataframe - dataframe has been created for C:\Users\saura\Desktop\pyspark_check\landing_zone\promotions\
2024-06-20 00:34:41,650 - Utils - INFO - utils - get_source_dataframe - dropping null records if any for C:\Users\saura\Desktop\pyspark_check\landing_zone\promotions\
2024-06-20 00:34:41,658 - Utils - INFO - utils - get_source_dataframe - dropping null records if any has been completed for C:\Users\saura\Desktop\pyspark_check\landing_zone\promotions\
2024-06-20 00:34:41,659 - Utils - INFO - utils - get_schema_from_json_file - trying to get schema for C:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\products_schema\schema_products.json
2024-06-20 00:34:41,722 - Utils - INFO - utils - get_schema_from_json_file - schema has beeen created forC:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\products_schema\schema_products.json
2024-06-20 00:34:41,723 - Utils - INFO - utils - get_source_dataframe - trying to create dataframe forC:\Users\saura\Desktop\pyspark_check\landing_zone\products\
2024-06-20 00:34:41,772 - Utils - INFO - utils - get_source_dataframe - dataframe has been created for C:\Users\saura\Desktop\pyspark_check\landing_zone\products\
2024-06-20 00:34:41,772 - Utils - INFO - utils - get_source_dataframe - dropping null records if any for C:\Users\saura\Desktop\pyspark_check\landing_zone\products\
2024-06-20 00:34:41,784 - Utils - INFO - utils - get_source_dataframe - dropping null records if any has been completed for C:\Users\saura\Desktop\pyspark_check\landing_zone\products\
2024-06-20 00:34:41,784 - Utils - INFO - utils - get_schema_from_json_file - trying to get schema for C:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\transactions_schema\schema_transactions.json
2024-06-20 00:34:41,859 - Utils - INFO - utils - get_schema_from_json_file - schema has beeen created forC:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\transactions_schema\schema_transactions.json
2024-06-20 00:34:41,859 - Utils - INFO - utils - get_source_dataframe - trying to create dataframe forC:\Users\saura\Desktop\pyspark_check\landing_zone\transactions\transactions.json
2024-06-20 00:34:41,886 - Utils - INFO - utils - get_source_dataframe - dataframe has been created for C:\Users\saura\Desktop\pyspark_check\landing_zone\transactions\transactions.json
2024-06-20 00:34:41,886 - Utils - INFO - utils - get_source_dataframe - dropping null records if any for C:\Users\saura\Desktop\pyspark_check\landing_zone\transactions\transactions.json
2024-06-20 00:34:41,895 - Utils - INFO - utils - get_source_dataframe - dropping null records if any has been completed for C:\Users\saura\Desktop\pyspark_check\landing_zone\transactions\transactions.json
2024-06-20 00:34:41,895 - Utils - INFO - utils - get_schema_from_json_file - trying to get schema for C:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\return_products_schema\schema_return_products.json
2024-06-20 00:34:41,968 - Utils - INFO - utils - get_schema_from_json_file - schema has beeen created forC:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\return_products_schema\schema_return_products.json
2024-06-20 00:34:41,968 - Utils - INFO - utils - get_source_dataframe - trying to create dataframe forC:\Users\saura\Desktop\pyspark_check\landing_zone\return_products\return_products.json
2024-06-20 00:34:41,984 - Utils - INFO - utils - get_source_dataframe - dataframe has been created for C:\Users\saura\Desktop\pyspark_check\landing_zone\return_products\return_products.json
2024-06-20 00:34:41,984 - Utils - INFO - utils - get_source_dataframe - dropping null records if any for C:\Users\saura\Desktop\pyspark_check\landing_zone\return_products\return_products.json
2024-06-20 00:34:41,991 - Utils - INFO - utils - get_source_dataframe - dropping null records if any has been completed for C:\Users\saura\Desktop\pyspark_check\landing_zone\return_products\return_products.json
2024-06-20 00:34:43,554 - Data_transformations - INFO - data_transformations - get_common_product_set - started calculating common sets of products that appears together
2024-06-20 00:34:43,715 - root - INFO - driver - main - printing product_list_df dataframe
2024-06-20 00:34:44,581 - Data_transformations - INFO - data_transformations - get_final_transactions - started calculating the final transactions dataframe
2024-06-20 00:34:44,754 - Data_transformations - ERROR - data_transformations - get_final_transactions - error has occured [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve "(product_id = applicable_products)" due to data type mismatch: the left and right operands of the binary operator have incompatible types ("STRING" and "ARRAY<STRING>").;
'Join LeftOuter, ((product_id#270 = applicable_products#25) AND (membership_level#5 = membership_level#26))
:- Project [product_id#270, customer_id#61, transaction_id#60, date#62, quantity#271, name#4, membership_level#5, geographic_region#6, purchase_history#7, description#42, attribute#43, price#44, tax_rate#45, (price#44 * cast(quantity#271 as double)) AS total_price_before_dis_tax#299]
:  +- Project [product_id#270, customer_id#61, transaction_id#60, date#62, quantity#271, name#4, membership_level#5, geographic_region#6, purchase_history#7, description#42, attribute#43, price#44, tax_rate#45]
:     +- Join Inner, (product_id#270 = product_id#41)
:        :- Project [customer_id#61, transaction_id#60, date#62, product_id#270, quantity#271, name#4, membership_level#5, geographic_region#6, purchase_history#7]
:        :  +- Join Inner, (customer_id#61 = customer_id#3)
:        :     :- Project [transaction_id#60, customer_id#61, date#62, item#264.product_id AS product_id#270, item#264.quantity AS quantity#271]
:        :     :  +- Project [transaction_id#60, customer_id#61, date#62, items#63, item#264]
:        :     :     +- Generate explode(items#63), false, [item#264]
:        :     :        +- Filter atleastnnonnulls(4, transaction_id#60, customer_id#61, date#62, items#63)
:        :     :           +- Relation [transaction_id#60,customer_id#61,date#62,items#63] json
:        :     +- Filter atleastnnonnulls(5, customer_id#3, name#4, membership_level#5, geographic_region#6, purchase_history#7)
:        :        +- Relation [customer_id#3,name#4,membership_level#5,geographic_region#6,purchase_history#7] json
:        +- Filter atleastnnonnulls(5, product_id#41, description#42, attribute#43, price#44, tax_rate#45)
:           +- Relation [product_id#41,description#42,attribute#43,price#44,tax_rate#45] json
+- Filter atleastnnonnulls(5, promo_id#22, description#23, discount#24, applicable_products#25, membership_level#26)
   +- Relation [promo_id#22,description#23,discount#24,applicable_products#25,membership_level#26] json

2024-06-20 00:34:44,754 - root - INFO - driver - main - printing transactions_customers_products_promos_prices_returns_df dataframe
