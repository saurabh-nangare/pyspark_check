2024-06-20 11:15:20,333 - root - INFO - driver - main - creating spark session
2024-06-20 11:15:20,333 - Create_spark - INFO - create_spark - get_spark - started creating spark object
2024-06-20 11:15:32,366 - Create_spark - INFO - create_spark - get_spark - created spark session
2024-06-20 11:15:32,367 - root - INFO - driver - main - spark session has been created successfully
2024-06-20 11:15:32,367 - root - INFO - driver - main - trying to read source dataframe for cutomers
2024-06-20 11:15:32,367 - root - INFO - driver - main - trying to get schema for customer
2024-06-20 11:15:32,368 - Utils - INFO - utils - get_schema_from_json_file - trying to get schema for C:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\customers_schema\schema_customers.json
2024-06-20 11:15:37,631 - Utils - INFO - utils - get_schema_from_json_file - schema has beeen created forC:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\customers_schema\schema_customers.json
2024-06-20 11:15:37,631 - Utils - INFO - utils - get_source_dataframe - trying to create dataframe forC:\Users\saura\Desktop\pyspark_check\landing_zone\customers\customers.json
2024-06-20 11:15:37,691 - Utils - INFO - utils - get_source_dataframe - dataframe has been created for C:\Users\saura\Desktop\pyspark_check\landing_zone\customers\customers.json
2024-06-20 11:15:37,691 - Utils - INFO - utils - get_source_dataframe - dropping null records if any for C:\Users\saura\Desktop\pyspark_check\landing_zone\customers\customers.json
2024-06-20 11:15:37,723 - Utils - INFO - utils - get_source_dataframe - dropping null records if any has been completed for C:\Users\saura\Desktop\pyspark_check\landing_zone\customers\customers.json
2024-06-20 11:15:37,723 - Utils - INFO - utils - get_schema_from_json_file - trying to get schema for C:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\promotions_schema\schema_promotions.json
2024-06-20 11:15:37,841 - Utils - INFO - utils - get_schema_from_json_file - schema has beeen created forC:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\promotions_schema\schema_promotions.json
2024-06-20 11:15:37,841 - Utils - INFO - utils - get_source_dataframe - trying to create dataframe forC:\Users\saura\Desktop\pyspark_check\landing_zone\promotions\
2024-06-20 11:15:37,895 - Utils - INFO - utils - get_source_dataframe - dataframe has been created for C:\Users\saura\Desktop\pyspark_check\landing_zone\promotions\
2024-06-20 11:15:37,896 - Utils - INFO - utils - get_source_dataframe - dropping null records if any for C:\Users\saura\Desktop\pyspark_check\landing_zone\promotions\
2024-06-20 11:15:37,910 - Utils - INFO - utils - get_source_dataframe - dropping null records if any has been completed for C:\Users\saura\Desktop\pyspark_check\landing_zone\promotions\
2024-06-20 11:15:37,981 - Utils - INFO - utils - get_schema_from_json_file - trying to get schema for C:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\products_schema\schema_products.json
2024-06-20 11:15:38,090 - Utils - INFO - utils - get_schema_from_json_file - schema has beeen created forC:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\products_schema\schema_products.json
2024-06-20 11:15:38,090 - Utils - INFO - utils - get_source_dataframe - trying to create dataframe forC:\Users\saura\Desktop\pyspark_check\landing_zone\products\
2024-06-20 11:15:38,182 - Utils - INFO - utils - get_source_dataframe - dataframe has been created for C:\Users\saura\Desktop\pyspark_check\landing_zone\products\
2024-06-20 11:15:38,182 - Utils - INFO - utils - get_source_dataframe - dropping null records if any for C:\Users\saura\Desktop\pyspark_check\landing_zone\products\
2024-06-20 11:15:38,203 - Utils - INFO - utils - get_source_dataframe - dropping null records if any has been completed for C:\Users\saura\Desktop\pyspark_check\landing_zone\products\
2024-06-20 11:15:38,203 - Utils - INFO - utils - get_schema_from_json_file - trying to get schema for C:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\transactions_schema\schema_transactions.json
2024-06-20 11:15:38,312 - Utils - INFO - utils - get_schema_from_json_file - schema has beeen created forC:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\transactions_schema\schema_transactions.json
2024-06-20 11:15:38,312 - Utils - INFO - utils - get_source_dataframe - trying to create dataframe forC:\Users\saura\Desktop\pyspark_check\landing_zone\transactions\transactions.json
2024-06-20 11:15:38,337 - Utils - INFO - utils - get_source_dataframe - dataframe has been created for C:\Users\saura\Desktop\pyspark_check\landing_zone\transactions\transactions.json
2024-06-20 11:15:38,337 - Utils - INFO - utils - get_source_dataframe - dropping null records if any for C:\Users\saura\Desktop\pyspark_check\landing_zone\transactions\transactions.json
2024-06-20 11:15:38,348 - Utils - INFO - utils - get_source_dataframe - dropping null records if any has been completed for C:\Users\saura\Desktop\pyspark_check\landing_zone\transactions\transactions.json
2024-06-20 11:15:38,377 - Utils - INFO - utils - get_schema_from_json_file - trying to get schema for C:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\return_products_schema\schema_return_products.json
2024-06-20 11:15:38,490 - Utils - INFO - utils - get_schema_from_json_file - schema has beeen created forC:\Users\saura\Desktop\pyspark_check\properties\source_file_schema\return_products_schema\schema_return_products.json
2024-06-20 11:15:38,491 - Utils - INFO - utils - get_source_dataframe - trying to create dataframe forC:\Users\saura\Desktop\pyspark_check\landing_zone\return_products\return_products.json
2024-06-20 11:15:38,514 - Utils - INFO - utils - get_source_dataframe - dataframe has been created for C:\Users\saura\Desktop\pyspark_check\landing_zone\return_products\return_products.json
2024-06-20 11:15:38,514 - Utils - INFO - utils - get_source_dataframe - dropping null records if any for C:\Users\saura\Desktop\pyspark_check\landing_zone\return_products\return_products.json
2024-06-20 11:15:38,522 - Utils - INFO - utils - get_source_dataframe - dropping null records if any has been completed for C:\Users\saura\Desktop\pyspark_check\landing_zone\return_products\return_products.json
2024-06-20 11:15:41,395 - Data_transformations - INFO - data_transformations - get_common_product_set - started calculating common sets of products that appears together
2024-06-20 11:15:41,571 - root - INFO - driver - main - printing product_list_df dataframe
2024-06-20 11:15:42,677 - Data_transformations - INFO - data_transformations - get_final_transactions - started calculating the final transactions dataframe
2024-06-20 11:15:42,979 - Data_transformations - ERROR - data_transformations - get_final_transactions - error has occured [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `yearquantity` cannot be resolved. Did you mean one of the following? [`quantity`, `tax_rate`, `date`, `discount`, `product_id`].;
'Project [transaction_id#67, customer_id#68, product_id#291, date#69, 'yearquantity, name#4, membership_level#5, geographic_region#6, purchase_history#7, description#49, attribute#50, price#51, tax_rate#52, total_price_before_discount_tax#320, CASE WHEN isnull(discount#24) THEN cast(0 as double) ELSE discount#24 END AS discount#373]
+- Join LeftOuter, ((product_id#291 = applicable_products#39) AND (membership_level#5 = membership_level#26))
   :- Project [product_id#291, customer_id#68, transaction_id#67, date#69, quantity#292, name#4, membership_level#5, geographic_region#6, purchase_history#7, description#49, attribute#50, price#51, tax_rate#52, (price#51 * cast(quantity#292 as double)) AS total_price_before_discount_tax#320]
   :  +- Project [product_id#291, customer_id#68, transaction_id#67, date#69, quantity#292, name#4, membership_level#5, geographic_region#6, purchase_history#7, description#49, attribute#50, price#51, tax_rate#52]
   :     +- Join Inner, (product_id#291 = product_id#48)
   :        :- Project [customer_id#68, transaction_id#67, date#69, product_id#291, quantity#292, name#4, membership_level#5, geographic_region#6, purchase_history#7]
   :        :  +- Join Inner, (customer_id#68 = customer_id#3)
   :        :     :- Project [transaction_id#67, customer_id#68, date#69, item#284.product_id AS product_id#291, item#284.quantity AS quantity#292]
   :        :     :  +- Project [transaction_id#67, customer_id#68, date#69, items#70, year#80, item#284]
   :        :     :     +- Generate explode(items#70), false, [item#284]
   :        :     :        +- Project [transaction_id#67, customer_id#68, date#69, items#70, year(cast(date#69 as date)) AS year#80]
   :        :     :           +- Filter atleastnnonnulls(4, transaction_id#67, customer_id#68, date#69, items#70)
   :        :     :              +- Relation [transaction_id#67,customer_id#68,date#69,items#70] json
   :        :     +- Filter atleastnnonnulls(5, customer_id#3, name#4, membership_level#5, geographic_region#6, purchase_history#7)
   :        :        +- Relation [customer_id#3,name#4,membership_level#5,geographic_region#6,purchase_history#7] json
   :        +- Filter atleastnnonnulls(5, product_id#48, description#49, attribute#50, price#51, tax_rate#52)
   :           +- Relation [product_id#48,description#49,attribute#50,price#51,tax_rate#52] json
   +- Project [promo_id#22, description#23, discount#24, applicable_products#39, membership_level#26]
      +- Generate explode(applicable_products#25), false, [applicable_products#39]
         +- Filter atleastnnonnulls(5, promo_id#22, description#23, discount#24, applicable_products#25, membership_level#26)
            +- Relation [promo_id#22,description#23,discount#24,applicable_products#25,membership_level#26] json

2024-06-20 11:15:42,979 - root - INFO - driver - main - printing transactions_customers_products_promos_prices_returns_df dataframe
